{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipo 2:\n",
    "\n",
    "Álvaro Martín López\n",
    "\n",
    "Daniel Pérez Martínez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALIZADOR DE SENTIMIENTO (NLP con Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](resources/Images/0_Portada.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCCIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este informe se verá todo el proceso necesario para crear un analizador de sentimiento en Python. Este proceso comprende desde la búsqueda y exploración del conjunto de datos hasta la visualización del resultado final.\n",
    "\n",
    "En concreto, primero veremos alternativas para obtener datos de entrenamiento para nuestro modelo y cómo debemos ajustarlos para un correcto funcionamiento del mismo.\n",
    "\n",
    "Posteriormente, veremos el pre-procesamiento necesario en estos datos de cara al NLP, para lo que utilizaremos funcionalidades de las librerías [INSERTAR AQUÍ LO QUE VAYA A USAR]\n",
    "\n",
    "Una vez tengamos el conjunto de datos listo, procederemos a crear nuestro modelo y a entrenarlo con los datos obtenidos.\n",
    "\n",
    "Por último, realizaremos una predicción para comprobar la precisión de nuestro modelo.\n",
    "\n",
    "También se incluye un apartado con conclusiones y consideraciones varias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORACIÓN DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro modelo clasificador utiliza el paradigma del aprendizaje supervisado. Esto significa que tenemos que aportarle como entrada un conjunto de datos etiquetados. \n",
    "\n",
    "En nuestro caso, este conjunto de datos será una colección de frases con su correspondiente etiqueta indicando el sentimiento de la misma.\n",
    "\n",
    "Para ello, nos apoyaremos en kaggle.com. Kaggle, una subsidiaria de Google LLC, es una comunidad en línea de científicos de datos y profesionales del aprendizaje de máquinas. Kaggle permite a los usuarios encontrar y publicar conjuntos de datos, explorar y construir modelos en un entorno de ciencia de datos basado en la web, trabajar con otros científicos de datos e ingenieros de aprendizaje de máquinas, y participar en competiciones para resolver los desafíos de la ciencia de datos. \n",
    "\n",
    "Así pues, buscaremos algún conjunto de datos que nos valga, como https://www.kaggle.com/c/spanish-arilines-tweets-sentiment-analysis/data\n",
    "\n",
    "Este conjunto consiste en diversos tweets en español acerca de las experiencias de viajeros con distintas compañias. Como podemos ver en la preview de Kaggle, contamos con varias versiones que añaden campos extra, como si los tweets son respuestas a otros, las localizaciones de los mismo... En el caso que nos ocupa solo queremos el texto y la etiqueta, por lo que descargaremos la versión simple:\n",
    "\n",
    "![title](resources/Images/1_Kaggle_Preview.png)\n",
    "\n",
    "\n",
    "En la imagen superior podemos ver los porcentajes de tweets de cada tipo. Como se puede observar, las negativas son  más frecuentes que las neutrales o positivas, algo que puede afectar al modelo. También podemos ver que no hay valores nulos o que falten, algo que no sería deseable tampoco. Por último, vemos que aparte de \"text\" y \"airline_sentiment\", los campos que nos sirven para el caso que nos ocupa, también contamos con otros campos extra de los que nos desharemos a continuación.\n",
    "\n",
    "Sin embargo, no siempre se dispone de las herramientas de kaggle para poder ver estos datos pues, en muchas ocasiones, el conjunto de datos de entrada lo habremos generado nosotros mismos. Veamos cómo explorar estos datos a mano ayudandonos de la librería *pandas*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Trabajar en #Ryanair como #TMA: https://t.co/r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fri Nov 03 12:05:12 +0000 2017</td>\n",
       "      <td>926419989107798016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Iberia @FIONAFERRER Cuando gusten en Cancún s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun Nov 26 18:40:28 +0000 2017</td>\n",
       "      <td>934854385577943041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mexico City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sabiais que @Iberia te trata muy bien en santi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Dec 25 15:40:45 +0000 2017</td>\n",
       "      <td>945318406441635840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Nov 06 14:18:35 +0000 2017</td>\n",
       "      <td>927540721296568320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Jan 01 23:00:57 +0000 2018</td>\n",
       "      <td>947965901332197376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buenos Aires</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  is_reply  reply_count  retweet_count  \\\n",
       "0           neutral     False            0              0   \n",
       "1           neutral      True            0              0   \n",
       "2          negative     False            0              0   \n",
       "3          negative     False            0              0   \n",
       "4          positive      True            0              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  Trabajar en #Ryanair como #TMA: https://t.co/r...         NaN   \n",
       "1  @Iberia @FIONAFERRER Cuando gusten en Cancún s...         NaN   \n",
       "2  Sabiais que @Iberia te trata muy bien en santi...         NaN   \n",
       "3  NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...         NaN   \n",
       "4  @cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...         NaN   \n",
       "\n",
       "                    tweet_created            tweet_id tweet_location  \\\n",
       "0  Fri Nov 03 12:05:12 +0000 2017  926419989107798016            NaN   \n",
       "1  Sun Nov 26 18:40:28 +0000 2017  934854385577943041            NaN   \n",
       "2  Mon Dec 25 15:40:45 +0000 2017  945318406441635840            NaN   \n",
       "3  Mon Nov 06 14:18:35 +0000 2017  927540721296568320            NaN   \n",
       "4  Mon Jan 01 23:00:57 +0000 2018  947965901332197376            NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0                      Madrid  \n",
       "1                 Mexico City  \n",
       "2                      Madrid  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4                Buenos Aires  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('resources/tweets_public.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero nos desharemos de las columnas que no nos interesan para nuestro caso y cambiaremos el nombre de una a algo más accesible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Trabajar en #Ryanair como #TMA: https://t.co/r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@Iberia @FIONAFERRER Cuando gusten en Cancún s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sabiais que @Iberia te trata muy bien en santi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>@cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0   neutral  Trabajar en #Ryanair como #TMA: https://t.co/r...\n",
       "1   neutral  @Iberia @FIONAFERRER Cuando gusten en Cancún s...\n",
       "2  negative  Sabiais que @Iberia te trata muy bien en santi...\n",
       "3  negative  NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...\n",
       "4  positive  @cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['airline_sentiment','text']]\n",
    "data.rename(columns = {'airline_sentiment' : 'sentiment'}, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora si tenemos algún valor nulo o que no encaje aunque, como ya sabíamos, no los hay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sentiment, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull()[data.isnull().eq(True).any(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos el conjunto con una preparación inicial, utilizaremos herramientas que nos ofrece la librería *pandas* para visualizar gráficamente los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6ae33d4d50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.value_counts(data['sentiment']).plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la diferencia no es sumamente notoria y este informe pretende enseñar los pasos más que resultados admirables, no vamos a editar más el conjunto de datos aunque, de ser necesario, se eliminarían frases aleatorias de los tipos que correspondan para tener un conjunto más igualado. \n",
    "\n",
    "Es importante recalcar que esto último no siempre es necesario, pues las frecuencias de aparición de cada tipo a veces son unas mayores que otras de forma natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTA__: Es necesario remarcar que la precisión del modelo se verá en su mayor parte afectada por la calidad del conjunto de datos de entrada. Sin embargo, los datasets de sentimiento públicos para textos en español son bastante escasos y de una calidad media. Se avisa que la precisión final del modelo no será muy alta debido a esto. Más información en el apartado de consideraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE-PRECESAMIENTO DE LOS DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de introducir estos datos a nuestro modelo, debemos aplicarles un preprocesamiento. Estamos trabajando con lenguaje natural, el cual es vasto y variante, por lo que hay que tomar una serie de medidas para normalizar los datos. \n",
    "\n",
    "En nuestro caso, recorreremos nuestro conjunto transformando las frases a minúscula y cambiando las letras con tilde por las mismas sin tilde, ya que al coger los datos de tweets, debemos tener en cuenta que no todo el mundo escribe correctamente. Dejaremos solo las letras (esto es, quitar emojis, links, hashtags, puntuación..) de forma que solo queden palabras y, una vez hecho, las lematizaremos, es decir, obtendremos el lexema o raíz. Esto resulta muy útil para que tokens que la máquina podría considerar distintos a priori ( _corrió, corrimos_ ) sean considerados correctamente como un mismo token ( _correr_ ).\n",
    "\n",
    "Por último, eliminaremos las llamadas _stopwords_ , que vienen siendo palabras que suelen ser frecuentes pero no aportan nada de significado real, como pueden ser preposiciones, conjunciones...\n",
    "\n",
    "Empecemos entonces con el paso a minúsculas y la sustitución de tildes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Trabajar en #Ryanair como #TMA: https://t.co/r...</td>\n",
       "      <td>trabajar en #ryanair como #tma: https://t.co/r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@Iberia @FIONAFERRER Cuando gusten en Cancún s...</td>\n",
       "      <td>@iberia @fionaferrer cuando gusten en cancun s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sabiais que @Iberia te trata muy bien en santi...</td>\n",
       "      <td>sabiais que @iberia te trata muy bien en santi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...</td>\n",
       "      <td>nunca nunca nunca pidais el cafe de ryanair.\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text  \\\n",
       "0   neutral  Trabajar en #Ryanair como #TMA: https://t.co/r...   \n",
       "1   neutral  @Iberia @FIONAFERRER Cuando gusten en Cancún s...   \n",
       "2  negative  Sabiais que @Iberia te trata muy bien en santi...   \n",
       "3  negative  NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...   \n",
       "\n",
       "                                              tokens  \n",
       "0  trabajar en #ryanair como #tma: https://t.co/r...  \n",
       "1  @iberia @fionaferrer cuando gusten en cancun s...  \n",
       "2  sabiais que @iberia te trata muy bien en santi...  \n",
       "3  nunca nunca nunca pidais el cafe de ryanair.\\n...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_lower_and_remove_accents(sentence):\n",
    "    replacements = {\n",
    "        \"á\" : \"a\",\n",
    "        \"é\" : \"e\",\n",
    "        \"í\" : \"i\",\n",
    "        \"ó\" : \"o\",\n",
    "        \"ú\" : \"u\",\n",
    "    }\n",
    "    sentence = sentence.lower()\n",
    "    for i,j in replacements.items():\n",
    "        sentence = sentence.replace (i,j)\n",
    "    return sentence\n",
    "\n",
    "    \n",
    "data['tokens'] = data['text'].apply(to_lower_and_remove_accents)\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, definiremos una función que eliminará los hashtags, menciones y links. Para ello nos ayudaremos del módulo de expresiones regulares _re_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def only_words(sentence):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",sentence).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto es una prueba hecha por y para y aqu un link falso\n"
     ]
    }
   ],
   "source": [
    "test= \"Esto es una prueba hecha por @alvaro y @dani para #TIE y aquí un link falso: https://unaweb.com/tienda\"\n",
    "print(only_words(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Trabajar en #Ryanair como #TMA: https://t.co/r...</td>\n",
       "      <td>trabajar en como</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@Iberia @FIONAFERRER Cuando gusten en Cancún s...</td>\n",
       "      <td>cuando gusten en cancun se viaja y disfruta de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sabiais que @Iberia te trata muy bien en santi...</td>\n",
       "      <td>sabiais que te trata muy bien en santiago de c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...</td>\n",
       "      <td>nunca nunca nunca pidais el cafe de ryanair bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text  \\\n",
       "0   neutral  Trabajar en #Ryanair como #TMA: https://t.co/r...   \n",
       "1   neutral  @Iberia @FIONAFERRER Cuando gusten en Cancún s...   \n",
       "2  negative  Sabiais que @Iberia te trata muy bien en santi...   \n",
       "3  negative  NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...   \n",
       "\n",
       "                                              tokens  \n",
       "0                                   trabajar en como  \n",
       "1  cuando gusten en cancun se viaja y disfruta de...  \n",
       "2  sabiais que te trata muy bien en santiago de c...  \n",
       "3  nunca nunca nunca pidais el cafe de ryanair bu...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'] = data['tokens'].apply(only_words)\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTA__ : Según la intención del analizador, puede que no nos interese eliminar las menciones. Por ejemplo, si lo que queremos es obtener la opinión de distintas compañías de vuelo utilizando este dataset, nos interesa eliminar únicamente el símbolo \"@\" pero dejar el nombre \"iberia\", \"ryanair\"... Puesto que únicamente nos interesa mostrar cómo crear un modelo genérico, hemos optado por eliminar todas las menciones y hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, gracias a la expresión regular anterior hemos eliminado también los signos de puntuación. Las expresiones regulares resultan muy útiles a la hora de preprocesar texto y suponen una herramienta muy potente si se es capaz de adecuar a nuestras necesidades\n",
    "\n",
    "A continuación procederemos a lematizar nuestro texto, cambiar formas verbales a infinitivo, sustantivos y adjetivos a singular, etc. Para ello nos apoyaremos en la herramienta _spaCy_ . Esta herramienta es muy potente cuando se trata de procesar lenguaje natural. Es capaz incluso de realizar análisis semántico cuando el texto está correctamente escrito y además con una precisión asombrosa. Sin embargo, nuestros datos proceden de _tweets_ así que únicamente utilizaremos su lematizador.\n",
    "\n",
    "__NOTA__ : Una vez más, encontramos dificultades dado el idioma de nuestro texto. Sin embargo, _spaCy_ ofrece también un módulo en español, aunque no tan bueno como el inglés dada la complejidad de nuestro lenguaje. Este módulo puede ser descargado con: _(python -m spacy download es_core_news_md)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_md')\n",
    "\n",
    "def lematize(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    result = \"\"\n",
    "    for token in doc:\n",
    "        result += token.lemma_ + \" \"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque la lematización es muy útil en el NLP, es un proceso costoso y lento, por lo que debe considerarse su uso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Este ser uno test parir comprobar qué tal funcionar SpacY en español con alguno plural y alguno verbo comer correr y volar \n"
     ]
    }
   ],
   "source": [
    "test = \"Este era un test para comprobar qué tal funciona SpacY en español con algunos plurales y algunos verbos como corrieron y volaron\"\n",
    "print(lematize(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, como vemos, hay ciertas palabras que no lematiza correctamente. Por ejemplo, \"para\" lo interpreta como si proviniese del verbo \"parir\" y \"como\" del verbo \"comer\". Por esta razón, aplicaremos antes el filtro de _stopwords_ que la lematización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la eliminación de _stopwords_ usaremos de nuevo la librería _spaCy_ , que cuenta con una lista de _stopwords_ también en español que nos permitirá crear un filtro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "def remove_stop_words(sentence):\n",
    "    result = list()\n",
    "    if sentence != None:\n",
    "        sentence = list(sentence.split(\" \"))\n",
    "        result = list(\n",
    "        filter(lambda l: (l not in STOP_WORDS and len(l) > 2), sentence))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'funcionamiento', 'funcion', 'elimina', 'stopwords']\n"
     ]
    }
   ],
   "source": [
    "test = \"esto es un test para ver el funcionamiento de la funcion que elimina stopwords\"\n",
    "print(remove_stop_words(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así pues, eliminamos las _stopwords_ de nuestro conjunto de datos y, posteriormente, aplicamos la lematización, aunque debemos retocar la función para que funcione con listas en lugar de cadenas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Trabajar en #Ryanair como #TMA: https://t.co/r...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@Iberia @FIONAFERRER Cuando gusten en Cancún s...</td>\n",
       "      <td>[gusten, cancun, viaja, disfruta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sabiais que @Iberia te trata muy bien en santi...</td>\n",
       "      <td>[sabiais, santiago, chile, cambia, asiento, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...</td>\n",
       "      <td>[pidais, cafe, ryanair, vendan, bordo]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text  \\\n",
       "0   neutral  Trabajar en #Ryanair como #TMA: https://t.co/r...   \n",
       "1   neutral  @Iberia @FIONAFERRER Cuando gusten en Cancún s...   \n",
       "2  negative  Sabiais que @Iberia te trata muy bien en santi...   \n",
       "3  negative  NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...   \n",
       "\n",
       "                                              tokens  \n",
       "0                                                 []  \n",
       "1                  [gusten, cancun, viaja, disfruta]  \n",
       "2  [sabiais, santiago, chile, cambia, asiento, ma...  \n",
       "3             [pidais, cafe, ryanair, vendan, bordo]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'] = data['tokens'].apply(remove_stop_words)\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematize(sentence_list):\n",
    "    result = []\n",
    "    if len(sentence_list) > 0:\n",
    "        for word in sentence_list:\n",
    "            doc = nlp(word)\n",
    "            for token in doc:\n",
    "                result.append(token.lemma_)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Trabajar en #Ryanair como #TMA: https://t.co/r...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@Iberia @FIONAFERRER Cuando gusten en Cancún s...</td>\n",
       "      <td>[gustar, cancun, viajar, disfrutar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sabiais que @Iberia te trata muy bien en santi...</td>\n",
       "      <td>[sabiais, santiago, chile, cambiar, asentir, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...</td>\n",
       "      <td>[pidais, cafe, ryanair, vender, bordar]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text  \\\n",
       "0   neutral  Trabajar en #Ryanair como #TMA: https://t.co/r...   \n",
       "1   neutral  @Iberia @FIONAFERRER Cuando gusten en Cancún s...   \n",
       "2  negative  Sabiais que @Iberia te trata muy bien en santi...   \n",
       "3  negative  NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...   \n",
       "\n",
       "                                              tokens  \n",
       "0                                                 []  \n",
       "1                [gustar, cancun, viajar, disfrutar]  \n",
       "2  [sabiais, santiago, chile, cambiar, asentir, m...  \n",
       "3            [pidais, cafe, ryanair, vender, bordar]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'] = data['tokens'].apply(lematize)\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, eliminaremos aquellas oraciones que hayan quedado vacías tras el preprocesamiento y, una vez hecho esto, podremos comenzar a crear nuestro modelo clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@Iberia @FIONAFERRER Cuando gusten en Cancún s...</td>\n",
       "      <td>[gustar, cancun, viajar, disfrutar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sabiais que @Iberia te trata muy bien en santi...</td>\n",
       "      <td>[sabiais, santiago, chile, cambiar, asentir, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...</td>\n",
       "      <td>[pidais, cafe, ryanair, vender, bordar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>@cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...</td>\n",
       "      <td>[tortu, exito]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text  \\\n",
       "0   neutral  @Iberia @FIONAFERRER Cuando gusten en Cancún s...   \n",
       "1  negative  Sabiais que @Iberia te trata muy bien en santi...   \n",
       "2  negative  NUNCA NUNCA NUNCA pidáis el café de Ryanair.\\n...   \n",
       "3  positive  @cris_tortu @dakar @Iberia @Mitsubishi_ES @BFG...   \n",
       "\n",
       "                                              tokens  \n",
       "0                [gustar, cancun, viajar, disfrutar]  \n",
       "1  [sabiais, santiago, chile, cambiar, asentir, m...  \n",
       "2            [pidais, cafe, ryanair, vender, bordar]  \n",
       "3                                     [tortu, exito]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data['tokens'].map(lambda d: len(d)) > 0]\n",
    "data = data.reset_index()\n",
    "data = data.drop(columns={'index'})\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTA__: Obviamente las funciones anteriores se pueden agrupar dentro de una mayor que realice todo el preprocesamiento necesario a cada oración, pero de esta forma es más claro para explicar cada parte. Antes de continuar con la creación y entrenamiento del modelo debemos recordar que el preprocesamiento es una parte clave en el NLP y que, si bien cuanto más completo sea mejores resultados obtendremos, hay que adecuarlo a nuestras necesidades, pues puede suponer un coste en tiempo de computación bastante elevado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREACIÓN Y ENTRENAMIENTO DEL MODELO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICCIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZACIÓN Y CONCLUSION FINAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle - https://www.kaggle.com/\n",
    "\n",
    "Spanish Airlines Tweets Sentiment Analyis - https://www.kaggle.com/c/spanish-arilines-tweets-sentiment-analysis/data\n",
    "\n",
    "Pandas - https://pandas.pydata.org/\n",
    "\n",
    "Regex - https://docs.python.org/3/library/re.html\n",
    "\n",
    "SpaCy - https://spacy.io/\n",
    "\n",
    "SpaCY español - https://spacy.io/models/es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
